!pip install selenium webdriver_manager beautifulsoup4 pandas
!pip install selenium webdriver-manager beautifulsoup4 pandas tqdm

import urllib.request
import urllib.parse
import requests
import json
import time
from bs4 import BeautifulSoup
import pandas as pd
from tqdm import tqdm

# Naver Open API 인증 정보
client_id = 'jLS2rZZk8MgZUzQEG9DK'
client_secret = 'yi4XT5ylqs'

# 사용자로부터 축제 이름 입력
query = input("감성 분석할 축제 이름을 입력하세요 (예: 진해 군항제): ")
encText = urllib.parse.quote(query + " 후기")

# 수집할 글 개수 설정
display = 100  # 최대 100까지 가능
start = 1

# API 요청 URL 구성
url = f"https://openapi.naver.com/v1/search/blog?query={encText}&display={display}&start={start}"

# 요청 헤더 설정
headers = {
    "X-Naver-Client-Id": client_id,
    "X-Naver-Client-Secret": client_secret
}

# 요청 실행
request = urllib.request.Request(url, headers=headers)
response = urllib.request.urlopen(request)
rescode = response.getcode()

# 블로그 URL 목록 가져오기
naver_urls = []

if rescode == 200:
    response_body = response.read()
    items = json.loads(response_body.decode('utf-8'))['items']
    
    for item in items:
        link = item['link']
        if 'blog.naver.com' in link:
            naver_urls.append(link)
else:
    print("Error Code:", rescode)

print(f"✅ 블로그 링크 {len(naver_urls)}개 수집 완료")

# 블로그 본문 추출 함수
def extract_blog_text(blog_url):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        # 1차 요청
        res = requests.get(blog_url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, "html.parser")

        # iframe 내부 링크 찾기
        iframe = soup.find("iframe")
        if iframe:
            iframe_url = "https://blog.naver.com" + iframe["src"]
            res = requests.get(iframe_url, headers=headers, timeout=5)
            soup = BeautifulSoup(res.text, "html.parser")

        # 본문 추출
        content = soup.select_one('div.se-main-container')
        if not content:
            content = soup.select_one('div#postViewArea')
        return content.get_text(separator="\n").strip() if content else ""
    
    except Exception as e:
        print("❌ 오류:", e)
        return ""

# 본문 수집
texts = []
for url in tqdm(naver_urls):
    text = extract_blog_text(url)
    if text:
        texts.append(text)
    time.sleep(1)  # 과도한 요청 방지

print(f"\n✅ 본문 {len(texts)}개 수집 완료")

# DataFrame 저장
df = pd.DataFrame({
    'url': naver_urls[:len(texts)],
    'content': texts
})

df.to_csv(f'{query}_blogs.csv', index=False, encoding='utf-8-sig')
df.head()
