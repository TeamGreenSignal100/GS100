#ëœë¤ í¬ë ˆìŠ¤íŠ¸
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from google.colab import drive

# íŒŒì¼ ê²½ë¡œ ì„¤ì • (ê²½ë¡œëŠ” ì‚¬ìš©ìì˜ ë“œë¼ì´ë¸Œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
file_path_2023 = "/content/drive/MyDrive/á„ƒá…¢á„’á…¡á†¨á„€á…­/á„ƒá…¢á„’á…¡á†¨á„€á…­ 3á„’á…¡á†¨á„‚á…§á†«/á„ƒá…¢á„’á…¡á†¨á„€á…­ 3-1 á„€á…ªá„†á…©á†¨/á„á…¢á†¸á„‰á…³á„á…©á†«á„ƒá…µá„Œá…¡á„‹á…µá†«EDAá„‰á…¥á†¯á„€á…¨á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³_á„’á…ªá†¼á„á…¥á†¯á„’á…§á†«P/á„á…¢á†¸á„‰á…³á„á…©á†« á„‹á…µá†¯á„Œá…¡á„‡á…§á†¯ á„‰á…¡á†«á„á…®á†¯ á„‘á…¡á„‹á…µá†¯/2025 08 15_ api,á„‡á…µá†¨á„á…¡á„‹á…µá†«á„Œá…³,á„†á…®á†«á„á…¦á„‡á…® á„á…©á†¼á„’á…¡á†¸data/á„á…©á†¼á„’á…¡á†¸á„‡á…©á†«/2023_á„á…©á†¼á„’á…¡á†¸á„‡á…©á†«(á„†á…®á†«á„á…¦á„‡á…®+á„‡á…µá†¨á„á…¡á„‹á…µá†«á„Œá…³_á„‚á…¦á„‹á…µá„‡á…¥api).csv"
file_path_2024 = "/content/drive/MyDrive/á„ƒá…¢á„’á…¡á†¨á„€á…­/á„ƒá…¢á„’á…¡á†¨á„€á…­ 3á„’á…¡á†¨á„‚á…§á†«/á„ƒá…¢á„’á…¡á†¨á„€á…­ 3-1 á„€á…ªá„†á…©á†¨/á„á…¢á†¸á„‰á…³á„á…©á†«á„ƒá…µá„Œá…¡á„‹á…µá†«EDAá„‰á…¥á†¯á„€á…¨á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³_á„’á…ªá†¼á„á…¥á†¯á„’á…§á†«P/á„á…¢á†¸á„‰á…³á„á…©á†« á„‹á…µá†¯á„Œá…¡á„‡á…§á†¯ á„‰á…¡á†«á„á…®á†¯ á„‘á…¡á„‹á…µá†¯/2025 08 15_ api,á„‡á…µá†¨á„á…¡á„‹á…µá†«á„Œá…³,á„†á…®á†«á„á…¦á„‡á…® á„á…©á†¼á„’á…¡á†¸data/á„á…©á†¼á„’á…¡á†¸á„‡á…©á†«/2024_á„á…©á†¼á„’á…¡á†¸á„‡á…©á†«(á„†á…®á†«á„á…¦á„‡á…®+á„‡á…µá†¨á„á…¡á„‹á…µá†«á„Œá…³+á„‚á…¦á„‹á…µá„‡á…¥api).csv"

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df_2023 = pd.read_csv(file_path_2023)
df_2024 = pd.read_csv(file_path_2024)

# ì‚¬ìš©ì ì •ì˜ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜
def handle_missing_values(df):
    # ê° í–‰ì˜ ê²°ì¸¡ì¹˜ ê°œìˆ˜ ê³„ì‚°
    nan_counts = df.isnull().sum(axis=1)

    # ê²°ì¸¡ì¹˜ ê°œìˆ˜ê°€ 2ê°œ ì´ìƒì¸ í–‰ ì‚­ì œ
    df_cleaned = df[nan_counts <= 1]

    # ë‚˜ë¨¸ì§€ ê²°ì¸¡ì¹˜(1ê°œ ì´í•˜)ëŠ” 0ìœ¼ë¡œ ì±„ìš°ê¸°
    df_filled = df_cleaned.fillna(0)

    return df_filled

# 2023ë…„ê³¼ 2024ë…„ ë°ì´í„°ì— í•¨ìˆ˜ ì ìš©
df_2023 = handle_missing_values(df_2023)
df_2024 = handle_missing_values(df_2024)

# í•„ìš”í•œ ì—´ë§Œ ì„ íƒ
features = ['êµ­ë¹„', 'ì§€ë°©ë¹„', 'ê¸°íƒ€', 'í•©ê³„', 'ë‚´êµ­ì¸(ëª…)', 'ì™¸êµ­ì¸(ëª…)', 'ê°ì„±_ì ìˆ˜í‰ê· ', 'G101', 'G201', 'G202', 'G301', 'G401', 'G501', 'í¬ìŠ¤íŒ…_ë¹ˆë„', 'ê¸ì •_í‚¤ì›Œë“œìˆ˜', 'ë¶€ì •_í‚¤ì›Œë“œìˆ˜', 'ê¸ì •ë¬¸ì¥ë¹„ìœ¨', 'ì¤‘ë¦½ë¬¸ì¥ë¹„ìœ¨', 'ë¶€ì •ë¬¸ì¥ë¹„ìœ¨']
target = 'target'

# 2023ë…„ ë°ì´í„°(í•™ìŠµ ë°ì´í„°)
X_train = df_2023[features]
y_train = df_2023[target]

# 2024ë…„ ë°ì´í„°(í…ŒìŠ¤íŠ¸ ë°ì´í„°)
X_test = df_2024[features]
y_test = df_2024[target]

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (KNNì„ ìœ„í•´ í•„ìˆ˜, Decision TreeëŠ” ì„ íƒ ì‚¬í•­)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

df_2024.info()
df_2023.info()

from sklearn.ensemble import RandomForestClassifier

# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ì •ì˜
rf_model = RandomForestClassifier(random_state=42)

# í•™ìŠµ
rf_model.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred_rf = rf_model.predict(X_test)

# í‰ê°€ ì§€í‘œ ê³„ì‚°
print("=== ëœë¤ í¬ë ˆìŠ¤íŠ¸ ===")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Precision:", precision_score(y_test, y_pred_rf, average='weighted'))
print("Recall:", recall_score(y_test, y_pred_rf, average='weighted'))
print("F1 Score:", f1_score(y_test, y_pred_rf, average='weighted'))

# í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
cm = confusion_matrix(y_test, y_pred_rf)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_model.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("ëœë¤ í¬ë ˆìŠ¤íŠ¸ í˜¼ë™ í–‰ë ¬")
plt.show()


#--------------------------
#LightGBM

import lightgbm as lgb
from sklearn.metrics import (
    accuracy_score, f1_score, classification_report, confusion_matrix
)
# === 3. LightGBM ë¶„ë¥˜ ëª¨ë¸ íŠœë‹ ===
lgb_clf = lgb.LGBMClassifier(random_state=42)

param_grid_lgb = {
    'num_leaves': [31, 50],
    'learning_rate': [0.05, 0.1],
    'n_estimators': [100, 200]
}

grid_lgb = GridSearchCV(
    estimator=lgb_clf,
    param_grid=param_grid_lgb,
    scoring='f1',
    cv=3,
    n_jobs=-1,
    verbose=1
)

# 2023ë…„ ë°ì´í„°(X_2023_train, y_2023_train)ë¡œ ëª¨ë¸ í•™ìŠµ
grid_lgb.fit(X_train, y_train)
print("LightGBM ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:", grid_lgb.best_params_)

# === 5. ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡ ===
lgb_best_model = grid_lgb.best_estimator_

# 2024ë…„ ë°ì´í„°(X_2024_test)ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
lgb_pred_prob = lgb_best_model.predict_proba(X_test)[:, 1]

# í™•ë¥  â†’ í´ë˜ìŠ¤(0 ë˜ëŠ” 1)
lgb_pred = (lgb_pred_prob >= 0.5).astype(int)

# === 6. í‰ê°€ í•¨ìˆ˜ ===
def print_classification_metrics(y_true, y_pred, model_name):
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    print(f"\nğŸ“Š {model_name} ì„±ëŠ¥:")
    print(f"  Accuracy: {acc:.4f}")
    print(f"  F1 Score: {f1:.4f}")
    print("  Classification Report:")
    print(classification_report(y_true, y_pred))
    print("  Confusion Matrix:")
    print(confusion_matrix(y_true, y_pred))

# 2024ë…„ ë°ì´í„°(y_2024_test)ì™€ ì˜ˆì¸¡ê°’(lgb_pred) ë¹„êµ
print_classification_metrics(y_test, lgb_pred, "LightGBM")


#-------------------
#XGBoost
import pandas as pd
import numpy as np
from sklearn.model_selection import RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from google.colab import drive
from xgboost import XGBClassifier
from scipy.stats import uniform, randint

# íŒŒì¼ ê²½ë¡œ
file_path_2023 = "/content/drive/MyDrive/á„ƒá…¢á„’á…¡á†¨á„€á…­/á„ƒá…¢á„’á…¡á†¨á„€á…­ 3á„’á…¡á†¨á„‚á…§á†«/á„ƒá…¢á„’á…¡á†¨á„€á…­ 3-1 á„€á…ªá„†á…©á†¨/á„á…¢á†¸á„‰á…³á„á…©á†«á„ƒá…µá„Œá…¡á„‹á…µá†«EDAá„‰á…¥á†¯á„€á…¨á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³_á„’á…ªá†¼á„á…¥á†¯á„’á…§á†«P/á„á…¢á†¸á„‰á…³á„á…©á†« á„‹á…µá†¯á„Œá…¡á„‡á…§á†¯ á„‰á…¡á†«á„á…®á†¯ á„‘á…¡á„‹á…µá†¯/2025 08 15_ api,á„‡á…µá†¨á„á…¡á„‹á…µá†«á„Œá…³,á„†á…®á†«á„á…¦á„‡á…® á„á…©á†¼á„’á…¡á†¸data/á„á…©á†¼á„’á…¡á†¸á„‡á…©á†«/2023_á„á…©á†¼á„’á…¡á†¸á„‡á…©á†«(á„†á…®á†«á„á…¦á„‡á…®+á„‡á…µá†¨á„á…¡á„‹á…µá†«á„Œá…³_á„‚á…¦á„‹á…µá„‡á…¥api).csv"
file_path_2024 = "/content/drive/MyDrive/á„ƒá…¢á„’á…¡á†¨á„€á…­/á„ƒá…¢á„’á…¡á†¨á„€á…­ 3á„’á…¡á†¨á„‚á…§á†«/á„ƒá…¢á„’á…¡á†¨á„€á…­ 3-1 á„€á…ªá„†á…©á†¨/á„á…¢á†¸á„‰á…³á„á…©á†«á„ƒá…µá„Œá…¡á„‹á…µá†«EDAá„‰á…¥á†¯á„€á…¨á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³_á„’á…ªá†¼á„á…¥á†¯á„’á…§á†«P/á„á…¢á†¸á„‰á…³á„á…©á†« á„‹á…µá†¯á„Œá…¡á„‡á…§á†¯ á„‰á…¡á†«á„á…®á†¯ á„‘á…¡á„‹á…µá†¯/2025 08 15_ api,á„‡á…µá†¨á„á…¡á„‹á…µá†«á„Œá…³,á„†á…®á†«á„á…¦á„‡á…® á„á…©á†¼á„’á…¡á†¸data/á„á…©á†¼á„’á…¡á†¸á„‡á…©á†«/2024_á„á…©á†¼á„’á…¡á†¸á„‡á…©á†«(á„†á…®á†«á„á…¦á„‡á…®+á„‡á…µá†¨á„á…¡á„‹á…µá†«á„Œá…³+á„‚á…¦á„‹á…µá„‡á…¥api).csv"

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df_2023 = pd.read_csv(file_path_2023)
df_2024 = pd.read_csv(file_path_2024)

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
def handle_missing_values(df):
    nan_counts = df.isnull().sum(axis=1)
    df_cleaned = df[nan_counts <= 1]
    df_filled = df_cleaned.fillna(0)
    return df_filled

df_2023 = handle_missing_values(df_2023)
df_2024 = handle_missing_values(df_2024)

# í”¼ì²˜/íƒ€ê²Ÿ ì„ íƒ
features = [
    'êµ­ë¹„', 'ì§€ë°©ë¹„', 'ê¸°íƒ€', 'í•©ê³„', 'ë‚´êµ­ì¸(ëª…)', 'ì™¸êµ­ì¸(ëª…)',
    'ê°ì„±_ì ìˆ˜í‰ê· ', 'G101', 'G201', 'G202', 'G301', 'G401', 'G501',
    'í¬ìŠ¤íŒ…_ë¹ˆë„', 'ê¸ì •_í‚¤ì›Œë“œìˆ˜', 'ë¶€ì •_í‚¤ì›Œë“œìˆ˜',
    'ê¸ì •ë¬¸ì¥ë¹„ìœ¨', 'ì¤‘ë¦½ë¬¸ì¥ë¹„ìœ¨', 'ë¶€ì •ë¬¸ì¥ë¹„ìœ¨'
]
target = 'target'

X_train = df_2023[features]
y_train = df_2023[target]
X_test = df_2024[features]
y_test = df_2024[target]

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# RandomizedSearchCVìš© í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„í¬ (í™•ì¥)
param_dist = {
    'n_estimators': randint(100, 500),        # 100~500
    'learning_rate': uniform(0.01, 0.1),     # 0.01~0.11
    'max_depth': randint(3, 10),             # 3~9
    'subsample': uniform(0.6, 0.4),          # 0.6~1.0
    'colsample_bytree': uniform(0.6, 0.4),   # 0.6~1.0
    'gamma': uniform(0, 0.3),                # 0~0.3
    'reg_alpha': uniform(0, 0.5),            # L1 ê·œì œ
    'reg_lambda': uniform(1, 2),             # L2 ê·œì œ
    'min_child_weight': randint(1, 6),       # ìµœì†Œ ìì‹ ê°€ì¤‘ì¹˜
}

# XGBoost ëª¨ë¸
xgb = XGBClassifier(
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss',
    objective='binary:logistic' # Add this line to explicitly set the objective
)

# RandomizedSearchCV ì„¤ì • (ì†ë„+ì„±ëŠ¥ ê· í˜•)
random_search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=param_dist,
    n_iter=50,          # 50ë²ˆ ëœë¤ íƒìƒ‰
    scoring='f1',
    cv=3,               # 3-fold CV
    n_jobs=-1,
    verbose=2,
    random_state=42
)

# í•™ìŠµ
random_search.fit(X_train_scaled, y_train)

print("=== RandomizedSearchCV Results ===")
print("Best parameters found: ", random_search.best_params_)
print("Best cross-validated F1 Score: ", random_search.best_score_)

# ìµœì  ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
best_xgb = random_search.best_estimator_
y_pred_best = best_xgb.predict(X_test_scaled)

print("=== Optimized XGBoost Test Results ===")
print("Accuracy:", accuracy_score(y_test, y_pred_best))
print("Precision:", precision_score(y_test, y_pred_best))
print("Recall:", recall_score(y_test, y_pred_best))
print("F1 Score:", f1_score(y_test, y_pred_best))

# í˜¼ë™í–‰ë ¬ ì‹œê°í™”
cm = confusion_matrix(y_test, y_pred_best)
ConfusionMatrixDisplay(confusion_matrix=cm).plot(cmap=plt.cm.Blues)
plt.title("Optimized XGBoost Confusion Matrix")
plt.show()

# ì¤‘ìš” ë³€ìˆ˜ ì‹œê°í™”
importances = best_xgb.feature_importances_
feat_importance = pd.Series(importances, index=features).sort_values(ascending=False)

plt.figure(figsize=(10,6))
feat_importance.plot(kind='bar')
plt.title("Optimized XGBoost Feature Importance")
plt.show()

# ê¸°ì¡´ì— ì •ì˜ëœ importances ë° feat_importanceë¥¼ í™œìš©
importances = best_xgb.feature_importances_
feat_importance = pd.Series(importances, index=features).sort_values(ascending=False)

# ì¤‘ìš”ë„ë¥¼ ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
total_importance = feat_importance.sum()
importance_percentage = (feat_importance / total_importance) * 100

# ê²°ê³¼ ì¶œë ¥
print("=== Optimized XGBoost Feature Importance Percentage ===")
print(importance_percentage.to_string())

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# --- 1. XGBoost ë‚´ì¥ ì¤‘ìš”ë„ ë°±ë¶„ìœ¨ ê³„ì‚° ë° ì˜ë¬¸ ì´ë¦„ ë§¤í•‘ ---

# ê¸°ì¡´ì— ì •ì˜ëœ importances ë° feat_importanceë¥¼ í™œìš©
importances = best_xgb.feature_importances_

# ì¤‘ìš”ë„ ê³„ì‚° ì‹œ features ë¦¬ìŠ¤íŠ¸ëŠ” í•œê¸€ ì´ë¦„ì„ ê°€ì§€ê³  ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# (ì´ì „ ì…€ì—ì„œ XGBoost í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì›ë³¸ í”¼ì²˜ ì´ë¦„)
feat_importance_kr = pd.Series(importances, index=features).sort_values(ascending=False)

# ì¤‘ìš”ë„ë¥¼ ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
total_importance = feat_importance_kr.sum()
importance_percentage_kr = (feat_importance_kr / total_importance) * 100

# í”¼ì²˜ ì´ë¦„ í•œê¸€ -> ì˜ë¬¸ ë§¤í•‘ (ì´ì „ì— ì •ì˜ëœ feature_mapping í™œìš©)
feature_mapping = {
    'êµ­ë¹„': 'National_Fund',
    'ì§€ë°©ë¹„': 'Local_Fund',
    'ê¸°íƒ€': 'Other_Fund',
    'í•©ê³„': 'Total_Fund',
    'ë‚´êµ­ì¸(ëª…)': 'Korean_Visitors',
    'ì™¸êµ­ì¸(ëª…)': 'Foreign_Visitors',
    'ê°ì„±_ì ìˆ˜í‰ê· ': 'Sentiment_Score_Avg',
    'G101': 'Category_G101',
    'G201': 'Category_G201',
    'G202': 'Category_G202',
    'G301': 'Category_G301',
    'G401': 'Category_G401',
    'G501': 'Category_G501',
    'í¬ìŠ¤íŒ…_ë¹ˆë„': 'Posting_Frequency',
    'ê¸ì •_í‚¤ì›Œë“œìˆ˜': 'Positive_Keywords',
    'ë¶€ì •_í‚¤ì›Œë“œìˆ˜': 'Negative_Keywords',
    'ê¸ì •ë¬¸ì¥ë¹„ìœ¨': 'Positive_Sentence_Ratio',
    'ì¤‘ë¦½ë¬¸ì¥ë¹„ìœ¨': 'Neutral_Sentence_Ratio',
    'ë¶€ì •ë¬¸ì¥ë¹„ìœ¨': 'Negative_Sentence_Ratio'
}

# ì˜ë¬¸ ì´ë¦„ìœ¼ë¡œ Series Index ì—…ë°ì´íŠ¸ ë° ì •ë ¬
importance_percentage_en = importance_percentage_kr.rename(index=feature_mapping).sort_values(ascending=False)

print("=== Optimized XGBoost Feature Importance Percentage (ì˜ë¬¸) ===")
print(importance_percentage_en.to_string())

# --- 2. ì‹œê°í™” ---

plt.figure(figsize=(12, 8))
importance_percentage_en.plot(kind='barh', color='darkorange') # barhëŠ” ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„
plt.title('Optimized XGBoost Built-in Feature Importance (Gain %)', fontsize=15)
plt.xlabel('Importance Percentage (%) (Type: Gain)', fontsize=12)
plt.ylabel('Features', fontsize=12)
plt.gca().invert_yaxis() # ê°€ì¥ ì¤‘ìš”í•œ í”¼ì²˜ê°€ ìœ„ì— ì˜¤ë„ë¡ Yì¶• ìˆœì„œ ë°˜ì „
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# --- 1. XGBoost ë‚´ì¥ ì¤‘ìš”ë„ ë°±ë¶„ìœ¨ ê³„ì‚° ë° ì˜ë¬¸ ì´ë¦„ ë§¤í•‘ ---

# ê¸°ì¡´ì— ì •ì˜ëœ importances ë° feat_importanceë¥¼ í™œìš©
importances = best_xgb.feature_importances_

# ì¤‘ìš”ë„ ê³„ì‚° ì‹œ features ë¦¬ìŠ¤íŠ¸ëŠ” í•œê¸€ ì´ë¦„ì„ ê°€ì§€ê³  ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# (ì´ì „ ì…€ì—ì„œ XGBoost í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì›ë³¸ í”¼ì²˜ ì´ë¦„)
feat_importance_kr = pd.Series(importances, index=features).sort_values(ascending=False)

# ì¤‘ìš”ë„ë¥¼ ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
total_importance = feat_importance_kr.sum()
importance_percentage_kr = (feat_importance_kr / total_importance) * 100

# í”¼ì²˜ ì´ë¦„ í•œê¸€ -> ì˜ë¬¸ ë§¤í•‘ (ì´ì „ì— ì •ì˜ëœ feature_mapping í™œìš©)
feature_mapping = {
    'êµ­ë¹„': 'National_Fund',
    'ì§€ë°©ë¹„': 'Local_Fund',
    'ê¸°íƒ€': 'Other_Fund',
    'í•©ê³„': 'Total_Fund',
    'ë‚´êµ­ì¸(ëª…)': 'Korean_Visitors',
    'ì™¸êµ­ì¸(ëª…)': 'Foreign_Visitors',
    'ê°ì„±_ì ìˆ˜í‰ê· ': 'Sentiment_Score_Avg',
    'G101': 'Category_G101',
    'G201': 'Category_G201',
    'G202': 'Category_G202',
    'G301': 'Category_G301',
    'G401': 'Category_G401',
    'G501': 'Category_G501',
    'í¬ìŠ¤íŒ…_ë¹ˆë„': 'Posting_Frequency',
    'ê¸ì •_í‚¤ì›Œë“œìˆ˜': 'Positive_Keywords',
    'ë¶€ì •_í‚¤ì›Œë“œìˆ˜': 'Negative_Keywords',
    'ê¸ì •ë¬¸ì¥ë¹„ìœ¨': 'Positive_Sentence_Ratio',
    'ì¤‘ë¦½ë¬¸ì¥ë¹„ìœ¨': 'Neutral_Sentence_Ratio',
    'ë¶€ì •ë¬¸ì¥ë¹„ìœ¨': 'Negative_Sentence_Ratio'
}

# ì˜ë¬¸ ì´ë¦„ìœ¼ë¡œ Series Index ì—…ë°ì´íŠ¸ ë° ì •ë ¬
importance_percentage_en = importance_percentage_kr.rename(index=feature_mapping).sort_values(ascending=False)

print("=== Optimized XGBoost Feature Importance Percentage (ì˜ë¬¸) ===")
print(importance_percentage_en.to_string())

# --- 2. ì‹œê°í™” ---

plt.figure(figsize=(12, 8))
importance_percentage_en.plot(kind='barh', color='darkorange') # barhëŠ” ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„
plt.title('Optimized XGBoost Built-in Feature Importance (Gain %)', fontsize=15)
plt.xlabel('Importance Percentage (%) (Type: Gain)', fontsize=12)
plt.ylabel('Features', fontsize=12)
plt.gca().invert_yaxis() # ê°€ì¥ ì¤‘ìš”í•œ í”¼ì²˜ê°€ ìœ„ì— ì˜¤ë„ë¡ Yì¶• ìˆœì„œ ë°˜ì „
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# SHAP Explainer ìƒì„± (ì´ì „ ì½”ë“œì—ì„œ ì´ì–´ì§€ëŠ” ë¶€ë¶„)
explainer = shap.TreeExplainer(best_xgb)

# SHAP ê°’ ê³„ì‚° (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì‚¬ìš©)
shap_values = explainer.shap_values(X_test_scaled)

# ê° íŠ¹ì„±ë³„ SHAP ê°’ì˜ ì ˆëŒ€ í‰ê·  ê³„ì‚°
# np.abs()ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì„±ì˜ ê¸°ì—¬ ë°©í–¥(ì–‘ìˆ˜/ìŒìˆ˜)ê³¼ ê´€ê³„ì—†ì´ í¬ê¸°ë§Œ ê³ ë ¤í•©ë‹ˆë‹¤.
# np.mean(axis=0)ì„ ì‚¬ìš©í•˜ì—¬ ê° íŠ¹ì„±ë³„ í‰ê· ì„ êµ¬í•©ë‹ˆë‹¤.
importance_values = np.mean(np.abs(shap_values), axis=0)

# íŠ¹ì„± ì¤‘ìš”ë„ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì „ì²´ í•©ê³„ë¥¼ êµ¬í•©ë‹ˆë‹¤.
total_importance = np.sum(importance_values)

# ê° íŠ¹ì„±ë³„ ì¤‘ìš”ë„ ë¹„ìœ¨ ê³„ì‚°
importance_ratios = (importance_values / total_importance) * 100

# ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë§Œë“¤ì–´ ë³´ê¸° ì‰½ê²Œ ì •ë ¬í•©ë‹ˆë‹¤.
feature_importance_df = pd.DataFrame({
    'Feature': features,
    'SHAP Importance (%)': importance_ratios
}).sort_values(by='SHAP Importance (%)', ascending=False)

# ê²°ê³¼ ì¶œë ¥
print("=== SHAP-based Feature Importance Ratios ===")
print(feature_importance_df.to_string(index=False))

# ì¤‘ìš”ë„ ë¹„ìœ¨ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.
plt.figure(figsize=(12, 8))
plt.barh(feature_importance_df['Feature'], feature_importance_df['SHAP Importance (%)'], color='c')
plt.xlabel('Importance Ratio (%)')
plt.title('SHAP-based Feature Importance Ratios for XGBoost Model')
plt.gca().invert_yaxis()  # ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±ì´ ë§¨ ìœ„ì— ì˜¤ë„ë¡ yì¶•ì„ ë°˜ì „ì‹œí‚µë‹ˆë‹¤.
plt.show()

